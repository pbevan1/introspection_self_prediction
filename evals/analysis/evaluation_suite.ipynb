{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Suite\n",
    "This notebook looks at the evaluation suite.\n",
    "\n",
    "What are the ceilings and floors of the evaluation suite?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [ # ðŸ”µ within exp/\n",
    "    \"evaluation_suite\"\n",
    "]\n",
    "    \n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    (\"task\", \"set\"): [\"val\"],\n",
    "    # (\"language_model\", \"name\"): [\"number_triplets\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import io\n",
    "import contextlib\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/pbu5262/Documents/python_scripts/introspection_self_prediction to sys.path\n"
     ]
    }
   ],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family ['Univers Next Pro'] not found. Falling back to DejaVu Sans.\n"
     ]
    }
   ],
   "source": [
    "# do we have a nice font installed? You might need to clear the matplotlib font cache\n",
    "plt.rcParams[\"font.family\"] = fm.get_font(fm.findfont(\"Univers Next Pro\")).family_name # falls back to default automatically\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 dataframes in total\n"
     ]
    }
   ],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXP_DIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "clear_output()\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"object\") or config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 base and 0 self-prediction dataframes\n"
     ]
    }
   ],
   "source": [
    "object_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "meta_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(object_dfs)} base and {len(meta_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following datasets:\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('task', 'name')) for k in object_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following response properties:\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(\"We have the following response properties:\")\n",
    "response_properties = set([get_maybe_nested_from_dict(k, ('response_property', 'name')) for k in meta_dfs.keys()])\n",
    "print(response_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:esdaily-dialog:9FVdN4lB\": \"GPT3.5 trained only on daily-dialog\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:esenglish-words:9ETgwsBU\": \"GPT3.5 trained only on english-words\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:esnumber-triplets:9ERLbadu\": \"GPT3.5 trained only on number-triplets\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:eswikipedia:9EUWhp8h\": \"GPT3.5 trained only on wikipedia\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:esdear-abbie:9FWKR6B0\": \"GPT3.5 trained only on dear-abbie\",\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "    \"claude-3-sonnet-20240229\": \"Claude 3 Sonnet\",\n",
    "    \"claude-3-opus-20240229\": \"Claude 3 Opus\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOOR_CEILING_MODELS = {  # untrained model: [task specific tunes from that model]\n",
    "    \"GPT3.5\": [\n",
    "        \"GPT3.5 trained only on daily-dialog\",\n",
    "        \"GPT3.5 trained only on english-words\",\n",
    "        \"GPT3.5 trained only on number-triplets\",\n",
    "        \"GPT3.5 trained only on wikipedia\",\n",
    "        \"GPT3.5 trained only on dear-abbie\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models have labels\n"
     ]
    }
   ],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    label = \"\"\n",
    "    if isinstance(config, str):\n",
    "        config = eval(config)\n",
    "    model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "    if model in MODEL_LABELS:\n",
    "        model = MODEL_LABELS[model]\n",
    "    label += model\n",
    "    response_property = get_maybe_nested_from_dict(config, ('response_property', 'name'))\n",
    "    if response_property not in [\"None\", None]:\n",
    "        label += f\"\\n predicting {response_property}\"\n",
    "    note = get_maybe_nested_from_dict(config, 'note')\n",
    "    if note not in [\"None\", None]:\n",
    "        label += f\"\\n{note}\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mode_object_df(df: pd.DataFrame, response_property: str):\n",
    "    \"\"\"Takes in an object level df and returns a version where every response has been swapped out for the mode response in the dataframe. \n",
    "    This allows us to score how well the model would be at always meta-level predicting the mode. This corresponds to the model during finetuning learning to only predict the most common response, without learning any connection to the inputs\n",
    "    \"\"\"\n",
    "    # ensure that we're not changing the input df in-place\n",
    "    df = df.copy()\n",
    "    # get most common response property\n",
    "    mode = df[df['compliance'] == True][response_property].apply(clean_string).mode()[0] # if multiple most common answers, chooses one\n",
    "    mode_row = df[df[response_property].apply(clean_string) == mode].head(1)\n",
    "    # ensure that the mode row has the cleaned string\n",
    "    mode_row[response_property] = mode\n",
    "    # drop the input string\n",
    "    mode_row = mode_row.drop(\"string\", axis=1).drop(\"compliance\", axis=1)\n",
    "    # replace the rest of every row with mode_row\n",
    "    for column in mode_row.columns:\n",
    "        df[column] = [mode_row[column].item()] * len(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_ITERATIONS = 100\n",
    "\n",
    "def make_pairwise_tables(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    baseline_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()]) # we compare the model against the baseline of \n",
    "    bootstrapped_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    for object_config, object_df in object_dfs.items():\n",
    "        for meta_config, meta_df in meta_dfs.items():\n",
    "            # compute joint df\n",
    "            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                continue\n",
    "            results.loc[str(meta_config), str(object_config)] = measure(joint_df)\n",
    "\n",
    "            # what would we see under the baseline of always picking the object-level mode?\n",
    "            # add the resopnse property if necessary\n",
    "            if not 'response_property' in object_df.columns:\n",
    "                lazy_add_response_property_to_object_level(object_df, object_config, meta_config.response_property.name)\n",
    "\n",
    "            # in some cases, we might not have a response property in the object_df. In this case, we need to add it\n",
    "            if not meta_config['response_property']['name'] in object_df.columns:\n",
    "                object_df = lazy_add_response_property_to_object_level(object_df, object_config, meta_config['response_property']['name'])\n",
    "\n",
    "            # modify the object-level df to always contain the mode\n",
    "            mode_object_df = construct_mode_object_df(object_df, meta_config['response_property']['name'])\n",
    "            # compute joint df\n",
    "            mode_joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                mode_object_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                continue\n",
    "            baseline_results.loc[str(meta_config), str(object_config)] = measure(mode_joint_df)\n",
    "\n",
    "            # we want to compute the 95%CI of the measure. We do this by bootstrapping over resampling the joint_df\n",
    "            bootstrapped_results.loc[str(meta_config), str(object_config)] = bootstrap_ci(joint_df, measure, BOOTSTRAP_ITERATIONS)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens eg. when we are reading in task.set==train dataframes, and only compare against val\n",
    "    # get list of cols\n",
    "    drop_cols = results.columns[results.isna().all(axis=0)]\n",
    "    # and rows too\n",
    "    drop_rows = results.index[results.isna().all(axis=1)]\n",
    "    # drop them\n",
    "    results = results.drop(columns=drop_cols)\n",
    "    results = results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # the saem for the baseline results\n",
    "    baseline_results.index = baseline_results.index.map(get_label)\n",
    "    baseline_results.columns = baseline_results.columns.map(get_label)\n",
    "    # drop nas\n",
    "    baseline_results = baseline_results.drop(columns=drop_cols)\n",
    "    baseline_results = baseline_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    baseline_results = baseline_results.sort_index(axis=0)\n",
    "    baseline_results = baseline_results.sort_index(axis=1)\n",
    "    # and the same for the bootstrapped results\n",
    "    bootstrapped_results.index = bootstrapped_results.index.map(get_label)\n",
    "    bootstrapped_results.columns = bootstrapped_results.columns.map(get_label)\n",
    "    # drop cols and rows\n",
    "    bootstrapped_results = bootstrapped_results.drop(columns=drop_cols)\n",
    "    bootstrapped_results = bootstrapped_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=0)\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=1)\n",
    "    assert results.shape == baseline_results.shape == bootstrapped_results.shape\n",
    "    assert results.columns.equals(baseline_results.columns) and results.index.equals(baseline_results.index)\n",
    "    assert results.columns.equals(bootstrapped_results.columns) and results.index.equals(bootstrapped_results.index)\n",
    "    return results, baseline_results, bootstrapped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}\n",
    "\n",
    "def filter_by_dataset_and_response_property(dfs, dataset, response_property):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset and get_maybe_nested_from_dict(config, ('response_property', 'name')) == response_property}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to see debugging output in the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ceiling/floor\n",
    "For each task x response property, we want to compute the floor and ceiling of the evaluation suite.\\\n",
    "**Floor**: how well does an untrained model do?\\\n",
    "**Ceiling**: how well does a model trained only on the task do (jointly trained on all response properties on the task)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_ceilings_df = pd.DataFrame(columns=[\"dataset\", \"response_property\", \"model\", \"floor\", \"ceiling\"])\n",
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        # Create a buffer to capture output\n",
    "        buffer = io.StringIO()\n",
    "        # Redirect stdout to the buffer\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, _, _ = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        \n",
    "        if len(results) == 0 or results.shape[0] == 0:# or results.max().max() == 0.0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        for model, trained_models in FLOOR_CEILING_MODELS.items():\n",
    "            model_long = model+\"\\n predicting \"+response_property+\"\\n\"\n",
    "            model = model+\"\\n\"\n",
    "            trained_models = [m+\"\\n predicting \"+response_property+\"\\n\" for m in trained_models]\n",
    "            trained_models = [m for m in trained_models if m in results.index]\n",
    "            floor = results.loc[model_long, model] # how well does the untrained model do at predicting itself?\n",
    "            # print(f\"Floor for {model}: {floor}\")\n",
    "            ceiling = results.loc[trained_models, model].max()\n",
    "            # print(f\"Ceiling for {model}: {ceiling}\")\n",
    "            floor_ceilings_df = pd.concat([floor_ceilings_df, pd.DataFrame({\"dataset\": [dataset], \"response_property\": [response_property], \"model\": [model], \"floor\": [floor], \"ceiling\": [ceiling]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>response_property</th>\n",
       "      <th>model</th>\n",
       "      <th>floor</th>\n",
       "      <th>ceiling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dataset, response_property, model, floor, ceiling]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floor_ceilings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        # Create a buffer to capture output\n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        # Redirect stdout to the buffer\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrap_results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        \n",
    "        if len(results) == 0 or results.shape[0] == 0:# or results.max().max() == 0.0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, annot=True, fmt=\".2f\", ax=ax)\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrap_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "        \n",
    "        # Check if all baseline results in each column are the same\n",
    "        for col in range(baseline_results.shape[1]):\n",
    "            if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "                raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='grey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        # ax.text(-0.2, -0.4, \"<Modeâ€“baseline\\nin chevrons>\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprob heatmap\n",
    "What is the logprob of the _first token_ of the correct answer under the metaâ€“level model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrapped_results = make_pairwise_tables(likelihood_of_correct_first_token, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "                \n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, ax=ax, fmt=\".3f\")\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrapped_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "        \n",
    "        # # Check if all baseline results in each column are the same\n",
    "        # for col in range(baseline_results.shape[1]):\n",
    "        #     if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "        #         raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='lightgrey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Mean log-prob of initial object-level response under meta-level model\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object vs object change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which response property do we want to use for the analysis?\n",
    "response_property = \"identity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        # fake having a meta level for this\n",
    "        faux_meta_level = filter_by_dataset(object_dfs, dataset)\n",
    "        for config in faux_meta_level.keys():\n",
    "            config['response_property'] = {'name': response_property}\n",
    "        results, _, _ = make_pairwise_tables(calc_accuracy, filter_by_dataset(object_dfs, dataset), faux_meta_level)\n",
    "        print(f\"Overlap between object-level completions for {dataset}\")\n",
    "        \n",
    "        mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "        # plt.xlabel(\"Scored against object-level\")\n",
    "        # plt.ylabel(\"Meta-level\")\n",
    "        plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "        plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Entropy of object-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: (df['compliance'] == True).mean()\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Compliance of object-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Compliance of meta-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For posterity\n",
    "Save the notebook as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Manually save the notebook before proceeding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_THIS_NB = REPO_DIR / \"evals\"/ \"analysis\" / \"evaluation_suite.ipynb\"\n",
    "for study_folder in STUDY_FOLDERS:\n",
    "    OUT_PATH = EXP_DIR / study_folder / \"evaluation_suite_plots.html\"\n",
    "    subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", PATH_THIS_NB, \"--output\", OUT_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
