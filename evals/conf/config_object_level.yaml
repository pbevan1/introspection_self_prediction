hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${exp_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO

defaults:
  - prompt: object_level/minimal
  - language_model: gpt-3.5-turbo
  - task: wikipedia

# Common Params
study_name: ???
study_dir: ${experiment_folder_location:/${study_name}}
exp_dir: ${study_dir}/${sanitize:object_level_${language_model.model}_${prompt.method}_prompt_${task.name}_${task.set}_task}_${note}_note
note: "" # natural language description of the note of this run. Can be anything.
seed: 0
limit: 500
n_samples: 1 # for each input string, how many samples to generate
strings_path: ~ # if set, use the strings from this file rather than generating them from the dataset
filter_strings_path: ~ # if this is set, use this to filter the strings

reset: false
logging: INFO
print_prompt_and_response: false
cache_dir: ${exp_dir}/cache
prompt_history_dir: ${exp_dir}/prompt_history

# Model
language_model:
  temperature: 0.0
  logprobs: 2

task:
  num: ${limit}
  set: ??? # either all, train, or val. If generating training data, use train. If comparing with meta level, use val.

# API
organization: OWAIN_ORG
anthropic_tag: ANTHROPIC_API_KEY
openai_tag: OPENAI_API_KEY
anthropic_num_threads: 12
openai_fraction_rate_limit: 0.9
