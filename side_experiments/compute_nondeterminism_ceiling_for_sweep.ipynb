{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the ceiling performance for a model on the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR\n",
    "from evals.utils import run_command\n",
    "from evals.analysis.loading_data import get_hydra_config\n",
    "from evals.analysis.loading_data import load_single_df_from_exp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which models, and which tasks?\n",
    "Using the format from `scripts/sweep_full_study.py`.\n",
    "\n",
    "`TASKS` is a string of a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"may20_thrifty_sweep\"\n",
    "MODELS = [\n",
    "    \"claude-3-sonnet\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_sweep_9R9Lqsm2\", #ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9R9Lqsm2\",\n",
    "    \"finetuned/may20_thrifty_sweep/claude-3-sonnet/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_sweep_9R9L0Ddt\",\n",
    "    \"finetuned/may20_thrifty_sweep/gpt-4/ft_gpt-4-0613_dcevals-kokotajlo_sweep_9RSQ9BDP\",\n",
    "    \"finetuned/may20_thrifty_sweep/claude-3-sonnet/ft_gpt-4-0613_dcevals-kokotajlo_sweep_9RSQHCmp\",\n",
    "    \"finetuned/may20_thrifty_sweep/gpt-4/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_sweep_9RSPteWA\",\n",
    "    \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo/ft_gpt-4-0613_dcevals-kokotajlo_sweep_9RSPjTJF\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_lr2_9RW1QKsf\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo-0125/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_sweep_9Th6cCBF\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-4-turbo/ft_gpt-3.5-turbo-0125_dcevals-kokotajlo_sweep_9ThUFr7R\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-4-turbo/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_sweep_9ThBY0oK\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo-0125/ft_gpt-3.5-turbo-0125_dcevals-kokotajlo_sweep_9Th7D4TK\",\n",
    "    \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo/ft_gpt-3.5-turbo-0125_dcevals-kokotajlo_sweep_9ThVmSp2\",\n",
    "    \"finetuned/may20_thrifty_sweep/claude-3-sonnet/ft_gpt-3.5-turbo-0125_dcevals-kokotajlo_sweep_9Th9i5Mf\",\n",
    "    # \"finetuned/may20_thrifty_sweep/gpt-3.5-turbo/ft_gpt-3.5-turbo-1106_dcevals-kokotajlo_scramble_9TfFZ0nD\",\n",
    "]\n",
    "TASKS = '{\"number_triplets\": [\"identity\", \"is_even\", \"last_character\", \"first_character\"], \"wikipedia\": [\"identity\", \"syllable_count\", \"first_character\", \"last_character\"], \"writing_stories\": [\"identity\", \"first_word\", \"writing_stories/main_character_name\"], \"personal_preferences\": [\"identity\", \"syllable_count\", \"first_character\", \"last_character\"], }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other hyperparameters\n",
    "N_PER_TASK = 10\n",
    "SEED = 42\n",
    "# SAMPLES_PER_INPUT = 100\n",
    "SAMPLES_PER_INPUT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = eval(TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ceiling calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(MODELS):\n",
    "    for task in TASKS.keys():\n",
    "        # can we get the model divergent strings?\n",
    "        model_divergent_string_path = EXP_DIR / STUDY_NAME / f\"divergent_strings_{task}.csv\"\n",
    "        if os.path.exists(model_divergent_string_path):\n",
    "            print(f\"üîç Found divergent strings for {model} on {task}\")\n",
    "            command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_object_level.py study_name={'nondeterminism_ceiling/'+STUDY_NAME} task={task} language_model={model} task.set=val n_samples={SAMPLES_PER_INPUT} task.num={N_PER_TASK} strings_path={model_divergent_string_path} \"\n",
    "        else:\n",
    "            print(f\"üîç‚ö†Ô∏è Could not find divergent strings for {model} on {task}‚ÄîRunning without\")\n",
    "            command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_object_level.py study_name={'nondeterminism_ceiling/'+STUDY_NAME} task={task} language_model={model} task.set=val n_samples={SAMPLES_PER_INPUT} task.num={N_PER_TASK} \"\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Running {model} on {task}: {command}\")\n",
    "        run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the response properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = EXP_DIR / \"nondeterminism_ceiling\" / STUDY_NAME\n",
    "subfolders = [results_folder / f for f in next(os.walk(results_folder))[1]]\n",
    "print(f\"Got {len(subfolders)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in tqdm(subfolders):\n",
    "        # load config\n",
    "        try:\n",
    "                cfg = get_hydra_config(folder)\n",
    "        except ValueError:\n",
    "                print(f\"Skipping {folder}\")\n",
    "                continue\n",
    "        task = cfg.task.name\n",
    "        response_properties = TASKS[task]\n",
    "        for response_property in response_properties:\n",
    "                command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_property_extraction.py dir={folder} response_property={response_property}\"\n",
    "                print(f\"üõ∏ Extracting {response_property} on {model} on {task}: {command}\")\n",
    "                try:\n",
    "                        run_command(command)\n",
    "                except Exception as e:\n",
    "                        print(f\"Error: {e}\\nwhile running {command}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_match(df_subset, response_property='identity'):\n",
    "    # assert len(df_subset) == N_SAMPLES, f\"Expected {N_SAMPLES} samples, got {len(df_subset)}\"\n",
    "    assert df_subset['string'].nunique() == 1, \"Expected all samples to be from the same string\"\n",
    "    responses = df_subset[response_property].values\n",
    "    shuffled_responses = np.random.permutation(responses)\n",
    "    return np.mean(responses == shuffled_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd have to make them be the same distribution, but with different levels of noise. Seems harder. \n",
    "\n",
    "The way to do this would be: \n",
    "- for both pairs of responses\n",
    "    - find the most common response, rename 'A'\n",
    "    - find the second most common response, rename 'B'\n",
    "    - ...\n",
    "- see how often two arbitrary pairs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_match_across_sets(df_subsetA, df_subsetB, response_property='identity'):\n",
    "    assert df_subsetA['string'].nunique() == 1, \"Expected all samples to be from the same string\"\n",
    "    assert df_subsetB['string'].nunique() == 1, \"Expected all samples to be from the same string\"\n",
    "    # we need to replace the responses with dummy values‚Äîmost common one is 0, then 1, then 2, etc.\n",
    "    responsesA = df_subsetA[response_property].values\n",
    "    responsesB = df_subsetB[response_property].values\n",
    "    # count up the responses in A\n",
    "    response_countsA = {}\n",
    "    for response in responsesA:\n",
    "        if response not in response_countsA:\n",
    "            response_countsA[response] = 0\n",
    "        response_countsA[response] += 1\n",
    "    # sort by frequency\n",
    "    response_countsA = {k: v for k, v in sorted(response_countsA.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # count up the responses in B\n",
    "    response_countsB = {}\n",
    "    for response in responsesB:\n",
    "        if response not in response_countsB:\n",
    "            response_countsB[response] = 0\n",
    "        response_countsB[response] += 1\n",
    "    # sort by frequency\n",
    "    response_countsB = {k: v for k, v in sorted(response_countsB.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # make aligned responses\n",
    "    aligned_responsesA = []\n",
    "    for i, (response, count) in enumerate(response_countsA.items()):\n",
    "        aligned_responsesA.extend([i]*count)\n",
    "    aligned_responsesB = []\n",
    "    for i, (response, count) in enumerate(response_countsB.items()):\n",
    "        aligned_responsesB.extend([i]*count)\n",
    "    matches = []\n",
    "    for _ in range(BOOTSTRAP_N):\n",
    "        A = np.random.choice(aligned_responsesA)\n",
    "        B = np.random.choice(aligned_responsesB)\n",
    "        matches.append(A == B)\n",
    "    return np.mean(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_95_CI(samples):\n",
    "    means = []\n",
    "    for _ in range(BOOTSTRAP_N):\n",
    "        sample = np.random.choice(samples, len(samples), replace=True)\n",
    "        means.append(np.mean(sample))\n",
    "    return np.percentile(means, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ceiling(folder, response_property):\n",
    "    # load df\n",
    "    df = load_single_df_from_exp_path(folder, exclude_noncompliant=False) # TODO Should this be true? That might increase the ceiling.\n",
    "    samples_across_strings = []\n",
    "    means_across_strings = []\n",
    "\n",
    "    for string in tqdm(df.string.unique()):\n",
    "        samples_across_iters = [compute_pairwise_match(df[df.string == string], response_property) for _ in range(BOOTSTRAP_N)]\n",
    "        samples_across_strings.append(samples_across_iters)\n",
    "        means_across_strings.append(np.mean(samples_across_iters))\n",
    "    \n",
    "    all_samples = np.concatenate(samples_across_strings)\n",
    "    # return mean and 95%CI of mean\n",
    "    return np.mean(means_across_strings), bootstrap_95_CI(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ceiling_for_model_pair(folderA, folderB, response_property):\n",
    "    # load df\n",
    "    dfA = load_single_df_from_exp_path(folderA, exclude_noncompliant=False) # TODO Should this be true? That might increase the ceiling.\n",
    "    dfB = load_single_df_from_exp_path(folderB, exclude_noncompliant=False) # TODO Should this be true? That might increase the ceiling.\n",
    "    means_across_strings = []\n",
    "\n",
    "    for current_string in tqdm(set(list(dfA.string.unique()) + list(dfB.string.unique()))):\n",
    "        dfA_string_subset = dfA[dfA.string == current_string]\n",
    "        dfB_string_subset = dfB[dfB.string == current_string]\n",
    "        if len(dfA_string_subset) == 0 or len(dfB_string_subset) == 0:\n",
    "            continue\n",
    "        mean_acc = compute_pairwise_match_across_sets(dfA_string_subset, dfB_string_subset, response_property)\n",
    "        means_across_strings.append(mean_acc)\n",
    "    \n",
    "    # return mean and 95%CI of mean\n",
    "    return np.mean(means_across_strings), bootstrap_95_CI(means_across_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results = {}\n",
    "\n",
    "for folder in tqdm(subfolders):\n",
    "    try:\n",
    "        cfg = get_hydra_config(folder)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    task = cfg.task.name\n",
    "    model = cfg.language_model.model\n",
    "    response_properties = TASKS[task]\n",
    "    for response_property in response_properties:\n",
    "        mean, ci = compute_ceiling(folder, response_property)\n",
    "        ceiling_results[(model, task, response_property)] = (mean, ci)\n",
    "\n",
    "ceiling_results_df = pd.DataFrame(ceiling_results).T\n",
    "ceiling_results_df.columns = ['mean', 'ci']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated mean by model\n",
    "display(ceiling_results_df['mean'].groupby(level=0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to calculate how well each model predicts every other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd have to make them be the same distribution, but with different levels of noise. Seems harder. \n",
    "\n",
    "The way to do this would be: \n",
    "- for both pairs of responses\n",
    "    - find the most common response, rename 'A'\n",
    "    - find the second most common response, rename 'B'\n",
    "    - ...\n",
    "- see how often two arbitrary pairs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in folders in the structure {model}/{task}\n",
    "dfs_per_model_task = {}\n",
    "\n",
    "for folder in tqdm(subfolders):\n",
    "    try:\n",
    "        cfg = get_hydra_config(folder)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    task = cfg.task.name\n",
    "    model = cfg.language_model.model\n",
    "    try:\n",
    "        dfs_per_model_task[model][task] = folder\n",
    "    except KeyError:\n",
    "        dfs_per_model_task[model] = {task: folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results = {}\n",
    "\n",
    "for modelA in tqdm(dfs_per_model_task.keys()):\n",
    "    for task in dfs_per_model_task[modelA].keys():\n",
    "        folderA = dfs_per_model_task[modelA][task]\n",
    "        for modelB in dfs_per_model_task.keys():\n",
    "            folderB = dfs_per_model_task[modelB][task]\n",
    "            for response_property in TASKS[task]:\n",
    "                mean, ci = compute_ceiling_for_model_pair(folderA, folderB, response_property)\n",
    "                ceiling_pair_results[(modelA, modelB, task, response_property)] = {'mean': mean, 'ci': ci}\n",
    "\n",
    "ceiling_paired_results_df = pd.DataFrame(ceiling_pair_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ceiling_pair_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped by model pair\n",
    "display(ceiling_paired_results_df['mean'].groupby(level=[0,1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results_df = ceiling_paired_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results_df.columns = ['modelA', 'modelB', 'task', 'response_property', 'ceiling', 'ci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results_df.to_csv(EXP_DIR / \"nondeterminism_ceiling\" / f\"{STUDY_NAME}_ceiling_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_pair_results_df.to_csv(EXP_DIR / \"nondeterminism_ceiling\" / f\"{STUDY_NAME}_ceiling_pair_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
